\section{Methods}
We are dealing with a specific type of open-ended questions, where workers are supposed to provide a list of items
(in our application, items are potential influencers) . 
Among the provided items, some are of high-quality and some are not. Our goal is to find the items of high-quality. 
In this section, we introduce our proposed framework that we refer to as
\sys. We first formalize the problem then we introduce our method for
 measuring the influence of users and the quality of workers. 
 We then describe the overall model that learns from both parameters.
 We extend our model by introducing priors for our measure of worker's quality to
 capture our confidence  on this measure.
\subsection{Problem Definition}
\smallskip
\noindent\textbf{Notations.} Throughout this paper, we use boldface lowercase letters to denote vectors and boldface uppercase letters to denote matrices. For an arbitrary matrix $\mathbf{M}$, we use $\mathbf{M}_{i,j}$ to denote the entry at the $i$-th row and $j$-th column. We use capital letters (e.g., $\mathcal{P}$) in calligraphic math font to denote sets.

We use $\mathcal{I} = \{i_1, \ldots, i_n\}$ to denote the set of items, and  $\mathcal{J} = \{j_1, \ldots, j_m \}$ to denote the set of workers. For each item $i \in \mathcal{I}$, we have its features organized as a vector $\mathbf{x}_i$; and similarly, each worker $j \in \mathcal{J}$ has a feature vector $\mathbf{y}_j$. We use $\mathbf{A}_{i,j}=1$ to denote that item $i$ is an answer provided by worker $j$, and $\mathbf{A}_{i,j}=0$ otherwise. $\mathbf{A}_{i,j}$ is a sparse matrix where only a small proportion of the entries is non-zero. Our goal is find those items in $\mathcal{I}$ that are of high-quality, denoted by $\mathcal{SI} \subset \mathcal{I}$. We use $z_i = 1$ to denote $i\in \mathcal{SI}$, and $z_i = 0$ otherwise.

\smallskip
\noindent\textbf{Problem Statement.} Given $\mathbf{x}_i$ for each $i\in \mathcal{I}$ and $\mathbf{y}_j$ for each $j\in \mathcal{J}$, and the answer matrix $\mathbf{A}$, our goal is to infer $z_i$ for all $i\in \mathcal{I}$.
\subsection{Model}
To infer the quality of an item, our basic assumption is that answers provided by the crowd are not fully reliable. For each worker $j\in \mathcal{J}$, we use $r_j \in [0,1]$ to denote his/her reliability, where $r_j=1$ indicates the worker is fully reliable and $r_j=0$ indicates the worker is not reliable.
We assume that the item's quality $z_i$ is a random variable that can potentially take two values (i.e., 1 or 0), thus following a Bernoulli distribution:
\begin{equation}
    z_i \sim Bernoulli(\theta_i)
    \label{eq:dis_item}
\end{equation}

We assume that the quality of an item is related to its features. We formalize thus assumption by conditioning the distribution of $z_i$ on its features $x_i$:
\begin{equation}
    \theta_i  = \text{sigmoid} (f^{\mathcal{W}_I}(\mathbf{x}_i))
    \label{eq:item_features}
\end{equation}
where $f$ is a neural network, and $\mathcal{W}_I$ is the set of weight parameters of the neural network.

In our problem setting, the worker-item relation is valuable and worth to be exploited as if we know the reliability of the workers who name the item, then we have a better idea of the quality of the item. Similarly, worker reliability
is also related to which items they provide. For example, if we already know that most of the items a worker provides are of high-quality, then it is likely that this worker is reliable. 

To connect item quality and worker reliability through their naming relation, we assume that the worker reliability is further defined as the probability that the worker's answer is right:

\begin{equation}
    p(\mathbf{A}_{i,j} = z_i) =  r_j, p(\mathbf{A}_{i,j} \ne z_i) = 1- r_j
    \label{eq:item_worker_rel}
\end{equation}

If we consider workers' answers as a generative process, the above equation is equivalent to:
%we assume that a worker's answer $\mathbf{A}_{i,j}$ is conditioned on both quantities, formalized as below:
\begin{equation}
    p(\mathbf{A}_{i,j} | z_i,  r_j) = (1- r_j)^{|z_i-\mathbf{A}_{i,j}|}\times  r_j^{(1-|z_i-\mathbf{A}_{i,j}|)}
    \label{eq:item_worker}
\end{equation}

\subsection{EM Algorithm for \sys:} 
The EM algorithm iteratively takes two steps, i.e., the E-step and the
M-step. In each iteration, the E-step estimates the true labels of items given
the current parameters; the M-step then updates the estimation of
the parameters given the newly estimated true labels.

\noindent\textbf{E-step:}
From the Bayes rule, the item's quality is given by Equation~(\ref{eq:dist_cond}):
\begin{align}
    p(z_i|\mathbf{A}_{i,j},r_{j},x_i,\mathcal{W}_I)&=\frac{p(\mathbf{A}_{i,j}|z_i,r_j,x_i,\mathcal{W}_I)\times p(z_i|r_j,x_i,\mathcal{W}_I)}{p(\mathbf{A}_{i,j}|r_j,x_i,\mathcal{W}_I)}\nonumber\\
        &\propto p(\mathbf{A}_{i,j}|z_i,r_j)\times p(z_i|x_i,\mathcal{W}_I)
    \label{eq:dist_cond}
\end{align}
\iar{small latex problem the equation exceeds the column, I tried splitting the equation but did not look nice
what do you recommend?}
We assume workers answers are independent from each other and 
that item's quality $z_i$ depend only on the reliability of workers $j$ who named the
item $i$ (i.e, $\mathbf{A}_{i,j}=1$) hence Equation~(\ref{eq:dist_cond})
could be expressed as in Equation~(\ref{eq:e_step}):
\begin{equation}
    p(z_i) \propto \prod_{j \in \mathcal{J},\mathbf{A}_{i,j}=1} p(\mathbf{A}_{i,j}|z_i,r_{j})\times p(z_i|x_i,\mathcal{W}_I)
    \label{eq:e_step}
\end{equation}
Therefore, the item's quality is a function of both the worker-item relation and the item's quality with the priors.
The worker-item relation can be computed as in Equation~(\ref{eq:item_worker}) and the item's quality with 
the priors can be computed as in Equation~(\ref{eq:item_features}).

\noindent\textbf{M-step:}
Given the item's quality estimated by the E-step, we maximize
the following likelihood function to estimate the parameters:
\begin{align}
   \mathcal{L}
    &=\sum_{z_i}p(z_i)\log p(\mathbf{A_{i,j}},z_i|r_j,\mathbf{x}_i,\mathcal{W}_I)\nonumber\\
    &=\sum_{z_i}p(z_i)\log (p(\mathbf{A}_{i,j}| z_i , r_j) \times p(z_i |\mathbf{x}_i;\mathcal{W}_I)) \nonumber\\
    &=\sum_{z_i}p(z_i) (\log p(\mathbf{A}_{i,j}| z_i , r_j)+\log p(z_i |\mathbf{x}_i;\mathcal{W}_I)) \nonumber\\
    &=\underbrace{\sum_{z_i}p(z_i) \log p(\mathbf{A}_{i,j}| z_i , r_j)}_{\mathcal{M}_1}
    +\underbrace{\sum_{z_i}p(z_i)\log p(z_i |\mathbf{x}_i;\mathcal{W}_I)}_{\mathcal{M}_2}
    \label{eq:likelihood_m}
\end{align}
where $p(z_i)$ is obtained by the E-step (Equation~(\ref{eq:e_step})). With the obtained expression in 
Equation~(\ref{eq:likelihood_m}), the M-step can be decomposed in two parts independent from each other.
The first part namely $\mathcal{M}_1$ is solved via a stochastic gradient
ascent (SGA) method to update $r_j$.
\begin{equation}
    r_j=r_j+\eta \frac{\partial \mathcal{T}}{\partial r_j}
    \label{eq:m_rj}
\end{equation}
With $\frac{\partial \mathcal{T}}{\partial r_j}$ is given by Equation~(\ref{eq:grad_rj}):
\begin{equation}
    \frac{\partial \mathcal{T}}{\partial r_j}=\frac{p(\mathbf{A}_{i,j}=z_i)}{r_j}-\frac{p(\mathbf{A}_{i,j}\neq z_i)}{1-r_j}
    \label{eq:grad_rj}
\end{equation}
The second part namely $\mathcal{M}_2$  is exactly the same as the objective function for 
training a Neural Network, i.e., the opposite of a cross-entropy loss function. 
It therefore can be optimized using a stochastic gradient descent.
\iar{please let me know if what is written is correct and/or requires more derivations}

\subsection{Modeling Worker Reliability with Priors}
We have assumed so far a fixed parameter $r_j$ for worker reliability. 
We would like to extend our model by measuring our confidence in this parameter.
For example, suppose worker 1 has named 5 real influencers and worker 2 has named 
3 real influencers and 2 random names. In this case, we should be more confident
regarding $r_1$ than $r_2$. We proceed by assuming a Beta distribution for $r_j$:
\begin{equation}
		r_j \sim Beta(A_j,B_j)
		\label{eq:rj_dist}
\end{equation}
With priors on $r_j$, the closed form solution  for EM updates is no longer possible.
We develop a meanfield variational approach to approximate the posterior distribution 
$p(z,r)$ over all hidden variables. 
The main idea in the variational EM framework \cite{bayes2008variational} is to approximate $p(z,r)$ 
by a variational distribution $q(z,r)$ and to assume that it factorizes over
the latent variables $z$ and $r$ as expressed in Equation~(\ref{eq:dist_fact}).
\begin{equation}
	q(z,r)=\prod_{i} q(z_i) \prod_j q(r_j)
	\label{eq:dist_fact}
\end{equation}

We further assume the forms of the factor functions q:
\begin{align}
	q(z_i)=Bernoulli(\phi_i) \\
	q(r_j)=Beta(\alpha_j,\beta_j)
\end{align}
where $\phi_i$, $\alpha_j$ and $\beta_j$ are variational parameters
used to perform optimization to minimize the KL-divergence $\mathbf{KL}(q||P)$, 
making q ‘close’ to p.

We can derive the update equations as follows:
For $z_i$, we have:
\begin{align}
    q(z_i=0)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (P(\mathbf{A}_{i,j}|z_i=0,r_j))]\}}				\nonumber \\ &\times P(z_i=0|x_i, \mathcal{W}_I)  \nonumber \\
                &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (1-r_j)]\}}\times (1-\theta_i) \nonumber \\
    q(z_i=1)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (P(\mathbf{A}_{i,j}|z_i=1,r_j))]\}}\nonumber \\ &\times P(z_i=1|x_i, \mathcal{W}_I)   \nonumber \\
                &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (r_j)]\}}\times \theta_i 
\label{eq:q_two_poss}                
\end{align}

The expectations in Equations~(\ref{eq:q_two_poss}) can be evaluated as described in Equations~(\ref{eq:expect}):
\begin{align}
    \mathbb{E}_{q(r_j)}[\log (1-r_j)]&= \Psi(\beta_j)-\Psi(\alpha_j+\beta_j) \nonumber \\
    \mathbb{E}_{q(r_j)}[\log (r_j)]&= \Psi(\alpha_j)-\Psi(\alpha_j+\beta_j)
    \label{eq:expect}
\end{align}
Therefore, the expressions in Equations~(\ref{eq:q_two_poss}) can be simplified to the following Equations~(\ref{eq:qzi}):
\begin{align}
    q(z_i=0)   &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\Psi(\beta_j)-\Psi(\alpha_j+\beta_j)\}}\times (1-\theta_i) \nonumber \\  
     q(z_i=1)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{ \Psi(\alpha_j)-\Psi(\alpha_j+\beta_j)\}}\times \theta_i  
     \label{eq:qzi}
\end{align}

We move now to the variational update for the worker's reliability $r_j$:
\begin{equation}
    q(r_j) \propto \prod_{i} \exp{\{\mathbb{E}_{q(z_i)}[\log{P(\mathbf{A}_{i,j}|r_j,z_i)}]\}} \times   {P(r_j|x_j,\mathcal{W}_J)}
\end{equation}
We distinguish two main cases:
\begin{itemize}
\item if $\mathbf{A}_{i,j}=1$:
\begin{equation}
  q(r_j)  \propto Beta(\alpha_j+\sum_{i,\mathbf{A}_{i,j}=1}  \theta_i,\beta_j+ \sum_{i,\mathbf{A}_{i,j}=1} (1 - \theta_i) )
  \label{eq:rj_aij_1}
\end{equation}
\item if $\mathbf{A}_{i,j}=0$:
\begin{equation}
 q(r_j)  \propto Beta(\alpha_j+\sum_{i,\mathbf{A}_{i,j}=0} (1 - \theta_i),\beta_j+ \sum_{i,\mathbf{A}_{i,j}=0} \theta_i)
   \label{eq:rj_aij_0}
\end{equation}
\end{itemize}
The update rules described above plays the role of the E-step in a variational EM approach. 
The M-step can be performed in a similar manner as in the previous section for $z_i$.
\iar{how to describe what we are doing with r here?}