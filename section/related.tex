%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we first briefly discusses related work on finding social influencers and then review existing answers aggregation methods. Existing methods for finding social influencers are mainly feature-based ones. Typical features that has been explored include meta-data features such as the number of followers and followees \cite{Lehmann2013,Cheng2014}, semantic features such as the topics of a candidate influencer's microposts \cite{riahi2012finding,wei2016learning}, or even the geo-location of a candidate influencer \cite{Cheng2014} \jie{more references}. These methods, however, all relies on expert labels which are difficult to obtain. Our work tackles the problem using a fundamentally different approach, i.e., through crowdsourced open-ended question-answering. Methodologically, our proposed framework also considers social features, however it is mainly designed to aggregate crowd answers, which are not supported by existing feature-based methods.


Answers aggregation is a central problem in crowdsourcing. Typical methods include majority voting \cite{sheng2008get} and those based on EM, which simultaneously estimate the true labels and parameters related to the annotation process. We have introduced and compared typical EM-based methods in Section~\ref{sec:result}, including the classic DS model proposed by \citeauthor{dawid1979maximum} (\citeyear{dawid1979maximum}), and the more recent ZenCrowd by \citeauthor{demartini2012zencrowd} (\citeyear{demartini2012zencrowd}) and GLAD by \citeauthor{whitehill2009whose} (\citeyear{whitehill2009whose}). Closer to our method is LFC proposed by \citeauthor{raykar2010learning} (\citeyear{raykar2010learning}), who consider to model worker reliability as a latent variable with a prior distribution. Unlike LFC, our proposed framework further incorporates existing labels and social features, thus extending the applicability of answer aggregation to open-ended question-answering tasks. We note that our method is also related to the ``learning-from-crowds'' line of research \cite{raykar2010learning,tian2012learning,yang2018leveraging}, which considers the classification problem with noisy labels contributed by the crowd. Our framework is different in that it only requires a subset of the data instances to be labeled; it can, therefore, be characterized as a semi-supervised learning-from-crowd approach. 