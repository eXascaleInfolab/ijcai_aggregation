%%%% ijcai19.tex

\typeout{Aggregation for open-ended task}

% These are the instructions for authors for IJCAI-19.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'
\usepackage{ijcai19}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\urlstyle{same}
\usepackage[T1]{fontenc}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{url}
\usepackage{xspace}
\usepackage[algo2e,titlenotnumbered,boxed,ruled,vlined,linesnumbered]{algorithm2e}
\newcommand{\sys}{****\xspace}
% the following package is optional:
%\usepackage{latexsym} 
\title{Asking the Crowd for Social Influencer Finding: an Open-Ended Answer Aggregation Approach}

% Single author syntax
\author{
Paper ID: XXXX
%    XI Lab
%    \affiliations
%    University of Fribourg, Switzerland\emails
%    firstname.lastname@unifr.ch
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai19-multiauthor.tex file for detailed instructions
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi
\newcommand{\iar}[1]{\textcolor{blue}{(*** @Ines: #1 ***)}}
\begin{document}

\maketitle

\begin{abstract}
Finding social influencers is a fundamental task for many online applications, ranging from brand marketing to opinion mining. Existing methods are mainly based on supervised  learning techniques, whose performance is heavily limited by the availability of expert labels (often limited). In contrast to experts, the crowd possesses more complete knowledge -- on an aggregated level -- of influencers in many domains, such as fashion and fitness. Individual crowd workers, however, only possess fragmented knowledge that is often of low-quality. In this paper, we investigate how to effectively find social influencers by asking the crowd in the form of open-ended questions. In particular, we develop an Expectation-Maximization (EM) learning framework and a variational inference algorithm, which are able to infer the true influencers from fragmented crowd contributions while learning the reliability of individual crowd workers. Experimental results on real-world datasets show that our proposed framework substantially improves the state of the art. 
\end{abstract}

\section{Introduction}
Social influence is increasingly becoming a subtle force that controls the dynamics of the social network. 
Finding users in a network with a social influence is becoming a fundamental 
task for several applications such as brand marketing
~\cite{bond201261,richardson2002mining,van2007new}, expert finding for question 
answering~\cite{riahi2012finding} and collaborative task execution~\cite{sun2014analyzing,miao2010generative}. These users are referred to as ``influencers"
and are recognized with a set properties such as a high number of 
followers, a regular feed about a certain topic and an ``authentic" content.
 Existing work tackles the problem from a network perspective where 
 several approaches have been proposed to find influencers by performing content analysis and/or link analysis. However, the task of finding influencers remains a challenging task as these influencers
exhibit several subjective properties such as the ``authenticity" of their
content. Such a property is hard to assess by machines nowadays.
The current techniques used to find influencers in a social network
can be broadly classified into two groups:
\begin{itemize}
\item Social Influence Analysis: This method consists in measuring the influence of each user 
based on their content in social media.
Those satisfying a set of properties are classified as influencers (e.g 
number of followers higher than 1000)
\cite{Cheng2014,Lehmann2013}
 
\item Expert identification: While the Social Influence Analysis
problem have been extensively studied, they are still not as reliable
as experts. For example, Zalando (a fashion online retailer)
relies on an influencer platform named ``Collabary"\footnote{\url{https://www.collabary.com}} to identify influencers.
\end{itemize}

This paper represents a step towards bridging the gap between these two classes by leveraging 
human workers for influencers identification. 
As a matter of fact, workers are able to generate results as 
\textit{accurate} as experts and as \textit{scalable} as automatic user classification methods.
We introduce a new technique called \sys
which combines social properties of a collected set of users with worker's reliability to identify influencers.
More specifically, we consider a task where the crowd is asked to name influencers in a predefined domain.
The collected answers contain both noisy and low-quality elements hence, it is important
to control the quality of these answers. This is a common problem in crowdsourcing 
and a lot of techniques have been proposed to address it.  A simple yet effective method is 
Majority Voting. This method infers the truth from the majority as it takes the answer given by the majority of 
workers. However, Majority Voting considers all workers equal while in reality workers have different reliabilities.
Several algorithms have been proposed to measure worker's reliability~\cite{demartini2012zencrowd,raykar2010learning,dawid1979maximum}. All these methods mainly focuses on three types of tasks
\cite{ZhengLLSC17}: decision-making task, single/multiple choice task or numeric tasks. In these tasks, the number of
possible answers is usually predefined for example, in decision-making tasks, the answer could be either True or False. 
However, our task is an open-ended task i.e. it requires content creation and the number of possible answers
 is very large therefore the aforementioned techniques do not apply. 

Assessing workers answers in this task is particularly challenging for several reasons:
First, each worker provides a set of users who could potentially be an influencer 
and distilling the real ``influencer" from this set is not trivial 
based solely on the properties extracted from social media as they exhibit subjective
properties.
Second, when a worker $w_{1}$ provide a set of influencers but does not include an 
influencer provided by worker $w_{2}$ there's no explicit disagreement between 
$w_{1}$ and $w_{2}$. As a matter of fact, not naming an influencer does not mean 
classifying her as not an influencer.  Third, the concept of worker's reliability in our case
do not only depend on their motivation to do the task but also on their knowledge about
influencers in a particular domain. 

To address these issues, this paper introduces an Expectation Maximization (EM) 
algorithm that learns from both worker's reliability and the influencer's properties.
We also extend our model with efficient parameter estimation via variational inference.
\iar{extend?}

 \sys make a number of key contributions including:
\begin{itemize}
\item  We introduce the problem of aggregating open-ended task for influencer finding
\item We also present an Expectation Maximization (EM)  for learning influencer's quality 
\item We show an extension of the method using Variational Inference (VI)
for learning worker's reliability in finding influencers of a specific domain leveraging their
social properties 
\item We conduct extensive experiments on real-world datasets, demonstrating that
our technique outperforms state of the art techniques
\end{itemize}
To the best of our knowledge, this work is the first to find inluencers from sparse and noisy 
crowd answers. Our proposed framework is a generic one applicable to any open-ended task
for expert/influencer finding in a variety of domains.

\section{Related Work}
In this section, we discuss relevant work on (a) Truth Inference and (b) Social Influence Analysis
\iar{is there a difference between using the term aggregation and truth inference?
which one is more correct in our case? also is  it better to use Social Influence Analysis
or should I be more specific and say user classification?}
\subsection{Truth Inference}
Li et al.~\cite{catd} proposed a weighted majority voting. In their technique, 
worker's quality is assessed by two measures: 
a probability that reflects her reliability and a confidence based on the number of tasks she
answered. This technique aims at assigning a higher quality for workers who give 
plenty of answers close to the truth.
In~\cite{dawid1979maximum}, the authors proposed a probabilistic model 
where a confusion matrix is used to model the worker's quality and it 
applies the EM framework to infer the truth. Similarly, ZenCrowd 
\cite{demartini2012zencrowd} applies an EM framework to compute the 
worker's quality modeled as a probability (a real number between 0 and 1).
An extension to ZenCrowd was proposed in~\cite{li2014resolving} where
worker's quality is modeled in a wider range (in $(-\infty,+\infty)$.
These methods aims at maximizing the likelihood that reliable workers tend to 
provide correct results.\iar{not confident about this last sentence}
More recent work~\cite{yin2017aggregating} uses variational autoencoders
to infer the true label  of objects in an unsupervised manner.
\iar{I am adding this as it is an IJCAI'17 but it is not clear to me what is the main 
contribution other then using autoencoders for aggregation}

\subsection{Social Influence Analysis}
%To enable our method to be able to identify real influencers, we incorporate in our model
%a neural network to learn influencer's properties. 
Several approaches have been proposed 
to measure the influence of users in a social media platforms.
\cite{Cheng2014} proposes a geo-spatial-driven approach to find experts 
by using the GPS coordinates of millions of Twitter users. In this approach, user's influence
is a combination of her local authority and her topical authority reflecting respectively
her influence on the audience close to her in distance and in a specific topic.
\cite{Lehmann2013} define a framework to classify users as influencers 
based on three main features: (1) their visibility measured by the number of followers and tweeter
lists containing them, (2) their tweeting activity measured by the number of tweets and retweets 
per day and (3) their topic focus measured by the number of articles tweeted 
about a specific topic. 
\cite{wei2016learning} develop a probabilistic method to find topic experts in Twitter. They
measure the influence of users based on her local relevance and three main types of relation: 
follower relation, user-list relation and list-list relation. 
They assign to each user a score based on the computed influence and
define the influencers as the top-N users with the highest scores.
\iar{help me to add limitations of these methods plus why we are better}  
\section{Methods}
We are dealing with a specific type of open-ended questions, where workers are supposed to provide a list of items
(in our application, items are potential influencers) . 
Among the provided items, some are of high-quality and some are not. Our goal is to find the items of high-quality. 
In this section, we introduce our proposed framework that we refer to as
\sys. We first formalize the problem then we introduce our method for
 measuring the influence of users and the quality of workers. 
 We then describe the overall model that learns from both parameters.
 We extend our model by introducing priors for our measure of worker's quality to
 capture our confidence  on this measure.
\subsection{Problem Definition}
\smallskip
\noindent\textbf{Notations.} Throughout this paper, we use boldface lowercase letters to denote vectors and boldface uppercase letters to denote matrices. For an arbitrary matrix $\mathbf{M}$, we use $\mathbf{M}_{i,j}$ to denote the entry at the $i$-th row and $j$-th column. We use capital letters (e.g., $\mathcal{P}$) in calligraphic math font to denote sets.

We use $\mathcal{I} = \{i_1, \ldots, i_n\}$ to denote the set of items, and  $\mathcal{J} = \{j_1, \ldots, j_m \}$ to denote the set of workers. For each item $i \in \mathcal{I}$, we have its features organized as a vector $\mathbf{x}_i$; and similarly, each worker $j \in \mathcal{J}$ has a feature vector $\mathbf{y}_j$. We use $\mathbf{A}_{i,j}=1$ to denote that item $i$ is an answer provided by worker $j$, and $\mathbf{A}_{i,j}=0$ otherwise. $\mathbf{A}_{i,j}$ is a sparse matrix where only a small proportion of the entries is non-zero. Our goal is find those items in $\mathcal{I}$ that are of high-quality, denoted by $\mathcal{SI} \subset \mathcal{I}$. We use $z_i = 1$ to denote $i\in \mathcal{SI}$, and $z_i = 0$ otherwise.

\smallskip
\noindent\textbf{Problem Statement.} Given $\mathbf{x}_i$ for each $i\in \mathcal{I}$ and $\mathbf{y}_j$ for each $j\in \mathcal{J}$, and the answer matrix $\mathbf{A}$, our goal is to infer $z_i$ for all $i\in \mathcal{I}$.
\subsection{Model}
To infer the quality of an item, our basic assumption is that answers provided by the crowd are not fully reliable. For each worker $j\in \mathcal{J}$, we use $r_j \in [0,1]$ to denote his/her reliability, where $r_j=1$ indicates the worker is fully reliable and $r_j=0$ indicates the worker is not reliable.
We assume that the item's quality $z_i$ is a random variable that can potentially take two values (i.e., 1 or 0), thus following a Bernoulli distribution:
\begin{equation}
    z_i \sim Bernoulli(\theta_i)
    \label{eq:dis_item}
\end{equation}

We assume that the quality of an item is related to its features. We formalize thus assumption by conditioning the distribution of $z_i$ on its features $x_i$:
\begin{equation}
    \theta_i  = \text{sigmoid} (f^{\mathcal{W}_I}(\mathbf{x}_i))
    \label{eq:item_features}
\end{equation}
where $f$ is a neural network, and $\mathcal{W}_I$ is the set of weight parameters of the neural network.

In our problem setting, the worker-item relation is valuable and worth to be exploited as if we know the reliability of the workers who name the item, then we have a better idea of the quality of the item. Similarly, worker reliability
is also related to which items they provide. For example, if we already know that most of the items a worker provides are of high-quality, then it is likely that this worker is reliable. 

To connect item quality and worker reliability through their naming relation, we assume that the worker reliability is further defined as the probability that the worker's answer is right:

\begin{equation}
    p(\mathbf{A}_{i,j} = z_i) =  r_j, p(\mathbf{A}_{i,j} \ne z_i) = 1- r_j
    \label{eq:item_worker_rel}
\end{equation}

If we consider workers' answers as a generative process, the above equation is equivalent to:
%we assume that a worker's answer $\mathbf{A}_{i,j}$ is conditioned on both quantities, formalized as below:
\begin{equation}
    p(\mathbf{A}_{i,j} | z_i,  r_j) = (1- r_j)^{|z_i-\mathbf{A}_{i,j}|}\times  r_j^{(1-|z_i-\mathbf{A}_{i,j}|)}
    \label{eq:item_worker}
\end{equation}

\subsection{EM Algorithm for \sys:} 
The EM algorithm iteratively takes two steps, i.e., the E-step and the
M-step. In each iteration, the E-step estimates the true labels of items given
the current parameters; the M-step then updates the estimation of
the parameters given the newly estimated true labels.

\noindent\textbf{E-step:}
From the Bayes rule, the item's quality is given by Equation~(\ref{eq:dist_cond}):
\begin{align}
    p(z_i|\mathbf{A}_{i,j},r_{j},x_i,\mathcal{W}_I)&=\frac{p(\mathbf{A}_{i,j}|z_i,r_j,x_i,\mathcal{W}_I)\times p(z_i|r_j,x_i,\mathcal{W}_I)}{p(\mathbf{A}_{i,j}|r_j,x_i,\mathcal{W}_I)}\nonumber\\
        &\propto p(\mathbf{A}_{i,j}|z_i,r_j)\times p(z_i|x_i,\mathcal{W}_I)
    \label{eq:dist_cond}
\end{align}
\iar{small latex problem the equation exceeds the column, I tried splitting the equation but did not look nice
what do you recommend?}
We assume workers answers are independent from each other and 
that item's quality $z_i$ depend only on the reliability of workers $j$ who named the
item $i$ (i.e, $\mathbf{A}_{i,j}=1$) hence Equation~(\ref{eq:dist_cond})
could be expressed as in Equation~(\ref{eq:e_step}):
\begin{equation}
    p(z_i) \propto \prod_{j \in \mathcal{J},\mathbf{A}_{i,j}=1} p(\mathbf{A}_{i,j}|z_i,r_{j})\times p(z_i|x_i,\mathcal{W}_I)
    \label{eq:e_step}
\end{equation}
Therefore, the item's quality is a function of both the worker-item relation and the item's quality with the priors.
The worker-item relation can be computed as in Equation~(\ref{eq:item_worker}) and the item's quality with 
the priors can be computed as in Equation~(\ref{eq:item_features}).

\noindent\textbf{M-step:}
Given the item's quality estimated by the E-step, we maximize
the following likelihood function to estimate the parameters:
\begin{align}
   \mathcal{L}
    &=\sum_{z_i}p(z_i)\log p(\mathbf{A_{i,j}},z_i|r_j,\mathbf{x}_i,\mathcal{W}_I)\nonumber\\
    &=\sum_{z_i}p(z_i)\log (p(\mathbf{A}_{i,j}| z_i , r_j) \times p(z_i |\mathbf{x}_i;\mathcal{W}_I)) \nonumber\\
    &=\sum_{z_i}p(z_i) (\log p(\mathbf{A}_{i,j}| z_i , r_j)+\log p(z_i |\mathbf{x}_i;\mathcal{W}_I)) \nonumber\\
    &=\underbrace{\sum_{z_i}p(z_i) \log p(\mathbf{A}_{i,j}| z_i , r_j)}_{\mathcal{M}_1}
    +\underbrace{\sum_{z_i}p(z_i)\log p(z_i |\mathbf{x}_i;\mathcal{W}_I)}_{\mathcal{M}_2}
    \label{eq:likelihood_m}
\end{align}
where $p(z_i)$ is obtained by the E-step (Equation~(\ref{eq:e_step})). With the obtained expression in 
Equation~(\ref{eq:likelihood_m}), the M-step can be decomposed in two parts independent from each other.
The first part namely $\mathcal{M}_1$ is solved via a stochastic gradient
ascent (SGA) method to update $r_j$.
\begin{equation}
    r_j=r_j+\eta \frac{\partial \mathcal{T}}{\partial r_j}
    \label{eq:m_rj}
\end{equation}
With $\frac{\partial \mathcal{T}}{\partial r_j}$ is given by Equation~(\ref{eq:grad_rj}):
\begin{equation}
    \frac{\partial \mathcal{T}}{\partial r_j}=\frac{p(\mathbf{A}_{i,j}=z_i)}{r_j}-\frac{p(\mathbf{A}_{i,j}\neq z_i)}{1-r_j}
    \label{eq:grad_rj}
\end{equation}
The second part namely $\mathcal{M}_2$  is exactly the same as the objective function for 
training a Neural Network, i.e., the opposite of a cross-entropy loss function. 
It therefore can be optimized using a stochastic gradient descent.
\iar{please let me know if what is written is correct and/or requires more derivations}

\subsection{Modeling Worker Reliability with Priors}
We have assumed so far a fixed parameter $r_j$ for worker reliability. 
We would like to extend our model by measuring our confidence in this parameter.
For example, suppose worker 1 has named 5 real influencers and worker 2 has named 
3 real influencers and 2 random names. In this case, we should be more confident
regarding $r_1$ than $r_2$. We proceed by assuming a Beta distribution for $r_j$:
\begin{equation}
		r_j \sim Beta(A_j,B_j)
		\label{eq:rj_dist}
\end{equation}
With priors on $r_j$, the closed form solution  for EM updates is no longer possible.
We develop a meanfield variational approach to approximate the posterior distribution 
$p(z,r)$ over all hidden variables. 
The main idea in the variational EM framework \cite{bayes2008variational} is to approximate $p(z,r)$ 
by a variational distribution $q(z,r)$ and to assume that it factorizes over
the latent variables $z$ and $r$ as expressed in Equation~(\ref{eq:dist_fact}).
\begin{equation}
	q(z,r)=\prod_{i} q(z_i) \prod_j q(r_j)
	\label{eq:dist_fact}
\end{equation}

We further assume the forms of the factor functions q:
\begin{align}
	q(z_i)=Bernoulli(\phi_i) \\
	q(r_j)=Beta(\alpha_j,\beta_j)
\end{align}
where $\phi_i$, $\alpha_j$ and $\beta_j$ are variational parameters
used to perform optimization to minimize the KL-divergence $\mathbf{KL}(q||P)$, 
making q ‘close’ to p.

We can derive the update equations as follows:
For $z_i$, we have:
\begin{align}
    q(z_i=0)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (P(\mathbf{A}_{i,j}|z_i=0,r_j))]\}}				\nonumber \\ &\times P(z_i=0|x_i, \mathcal{W}_I)  \nonumber \\
                &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (1-r_j)]\}}\times (1-\theta_i) \nonumber \\
    q(z_i=1)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (P(\mathbf{A}_{i,j}|z_i=1,r_j))]\}}\nonumber \\ &\times P(z_i=1|x_i, \mathcal{W}_I)   \nonumber \\
                &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\mathbb{E}_{q(r_j)}[\log (r_j)]\}}\times \theta_i 
\label{eq:q_two_poss}                
\end{align}

The expectations in Equations~(\ref{eq:q_two_poss}) can be evaluated as described in Equations~(\ref{eq:expect}):
\begin{align}
    \mathbb{E}_{q(r_j)}[\log (1-r_j)]&= \Psi(\beta_j)-\Psi(\alpha_j+\beta_j) \nonumber \\
    \mathbb{E}_{q(r_j)}[\log (r_j)]&= \Psi(\alpha_j)-\Psi(\alpha_j+\beta_j)
    \label{eq:expect}
\end{align}
Therefore, the expressions in Equations~(\ref{eq:q_two_poss}) can be simplified to the following Equations~(\ref{eq:qzi}):
\begin{align}
    q(z_i=0)   &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{\Psi(\beta_j)-\Psi(\alpha_j+\beta_j)\}}\times (1-\theta_i) \nonumber \\  
     q(z_i=1)    &\propto \prod_{j \in \mathcal{T}_{i}} \exp{\{ \Psi(\alpha_j)-\Psi(\alpha_j+\beta_j)\}}\times \theta_i  
     \label{eq:qzi}
\end{align}

We move now to the variational update for the worker's reliability $r_j$:
\begin{equation}
    q(r_j) \propto \prod_{i} \exp{\{\mathbb{E}_{q(z_i)}[\log{P(\mathbf{A}_{i,j}|r_j,z_i)}]\}} \times   {P(r_j|x_j,\mathcal{W}_J)}
\end{equation}
We distinguish two main cases:
\begin{itemize}
\item if $\mathbf{A}_{i,j}=1$:
\begin{equation}
  q(r_j)  \propto Beta(\alpha_j+\sum_{i,\mathbf{A}_{i,j}=1}  \theta_i,\beta_j+ \sum_{i,\mathbf{A}_{i,j}=1} (1 - \theta_i) )
  \label{eq:rj_aij_1}
\end{equation}
\item if $\mathbf{A}_{i,j}=0$:
\begin{equation}
 q(r_j)  \propto Beta(\alpha_j+\sum_{i,\mathbf{A}_{i,j}=0} (1 - \theta_i),\beta_j+ \sum_{i,\mathbf{A}_{i,j}=0} \theta_i)
   \label{eq:rj_aij_0}
\end{equation}
\end{itemize}
The update rules described above plays the role of the E-step in a variational EM approach. 
The M-step can be performed in a similar manner as in the previous section for $z_i$.
\iar{how to describe what we are doing with r here?}
\section{Experiments}

\subsection{Experimental Setup}

\subsection{Experiments about Model 1}

\subsection{Experiments about Model 2}

\section{Conclusion}



\section*{Acknowledgments}



%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai19}

\end{document}

